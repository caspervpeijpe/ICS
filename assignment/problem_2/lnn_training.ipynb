{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac408b5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a96d02d6346fa7399cf6da0111ce0937",
     "grade": false,
     "grade_id": "cell-b00828259c8e42e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# RO47019: Intelligent Control Systems Practical Assignment\n",
    "* Period: 2022-2023, Q3\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/500969\n",
    "* Instructor: Cosimo Della Santina (C.DellaSantina@tudelft.nl)\n",
    "* Teaching assistant: Ruben Martin Rodriguez (R.MartinRodriguez@student.tudelft.nl)\n",
    "* (c) TU Delft, 2023\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Remove `raise NotImplementedError()` afterwards. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in your names, student numbers, netID, and emails below.\n",
    "STUDENT_1_NAME = \"\"\n",
    "STUDENT_1_STUDENT_NUMBER = \"\"\n",
    "STUDENT_1_NETID = \"\"\n",
    "STUDENT_1_EMAIL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba32571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "042927213b84aa368aa3ea72caa4cb60",
     "grade": true,
     "grade_id": "cell-9f148ec62e0de49c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert STUDENT_1_NAME != \"\"\n",
    "assert STUDENT_1_STUDENT_NUMBER != \"\"\n",
    "assert STUDENT_1_NETID != \"\"\n",
    "assert STUDENT_1_EMAIL != \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af317a94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e76ef40fcc3f08a0484661497162a1a9",
     "grade": false,
     "grade_id": "cell-4ea391677951116c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions, and do *not* copy solutions from others. By submitting your solutions, you claim that you alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled Q&A hours to ask the TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for youthat you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c956945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "387d2c60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49dc3998b44dff0403c71832d0af0e51",
     "grade": false,
     "grade_id": "cell-ed88010142fb94bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lagrangian Neural Network (LNN) training\n",
    "\n",
    "**Author:** Maximilian StÃ¶lzle (M.W.Stolzle@tudelft.nl)\n",
    "\n",
    "This notebook will contain functions to assist with the training of the lagrangian neural network. The task assignment can be found in the notebook `task_2c-3_train_lnn.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374ce54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b7ed9b37c6921a6e275f1fcb7770533",
     "grade": false,
     "grade_id": "cell-7f7dcbca8d22d0b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from distutils.util import strtobool\n",
    "from flax.core import FrozenDict\n",
    "from functools import partial\n",
    "from jax.config import config as jax_config\n",
    "\n",
    "jax_config.update(\"jax_platform_name\", \"cpu\")  # set default device to 'cpu'\n",
    "jax_config.update(\"jax_enable_x64\", True)  # double precision\n",
    "import dill\n",
    "from flax.training.train_state import TrainState\n",
    "import jax\n",
    "from jax import debug, jit\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # progress bar\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "from jax_double_pendulum.utils import normalize_link_angles\n",
    "\n",
    "# import the learned discrete forward dynamics from lagrangian_nn.ipynb\n",
    "from ipynb.fs.full.lnn import (\n",
    "    discrete_forward_dynamics,\n",
    "    MassMatrixNN,\n",
    "    PotentialEnergyNN,\n",
    ")\n",
    "\n",
    "# define boolean to check if the notebook is run for the purposes of autograding\n",
    "AUTOGRADING = strtobool(os.environ.get(\"AUTOGRADING\", \"false\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d5a4b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1514e18b1bb0a70edab8b11a8951c0d1",
     "grade": false,
     "grade_id": "cell-3840a1fa6f53d588",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_datasets(\n",
    "    filepath: Path, rng: random.KeyArray, val_ratio: float = 0.2\n",
    ") -> Tuple[Dict[str, jnp.ndarray], Dict[str, jnp.ndarray]]:\n",
    "    \"\"\"\n",
    "    Loads the datasets from a .npz file.\n",
    "    The dataset needs to have the following entries:\n",
    "        - dt_ss: array of shape (N, ) containing the time step between the current and the next state [s]\n",
    "        - th_curr_ss: Array of shape (N, 2) containing the current link angles of the double pendulum. [rad]\n",
    "        - th_d_curr_ss: Array of shape (N, 2) containing the current link angular velocities of the double pendulum. [rad/s]\n",
    "        - th_next_ss: Array of shape (N, 2) containing the nextt link angles of the double pendulum. [rad]\n",
    "        - th_d_next_ss: Array of shape (N, 2) containing the next link angular velocities of the double pendulum. [rad/s]\n",
    "    Args:\n",
    "        filepath: Path to the .npz file.\n",
    "        rng: PRNG key for pseudo-random number generation.\n",
    "        val_ratio: Ratio of validation dataset with respect to the entire dataset size. Needs to be in interval [0, 1].\n",
    "    Returns:\n",
    "        train_ds: Dictionary containing the training dataset with the same keys as the original dataset.\n",
    "        val_ds: Dictionary containing the validation dataset with the same keys as the original dataset.\n",
    "    \"\"\"\n",
    "    assert 0.0 <= val_ratio <= 1.0, \"Validation ratio needs to be in interval [0, 1].\"\n",
    "\n",
    "    dataset = jnp.load(filepath)\n",
    "    num_samples = dataset[\"th_curr_ss\"].shape[0]\n",
    "\n",
    "    # Randomly split the dataset into a training set and a validation set\n",
    "    # Important: make use of the provided PRNG key\n",
    "    train_ds, val_ds = {}, {}\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd274ae",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56dec58e9925be9e4b15c83e86e10c1c",
     "grade": false,
     "grade_id": "cell-13ff45f8b460488f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_learning_rate_fn(\n",
    "    num_epochs: int, steps_per_epoch: int, base_lr: float, warmup_epochs: int = 0\n",
    ") -> Callable:\n",
    "    \"\"\"\n",
    "    Creates a learning rate schedule function. THe learning rate scheduler implements the following procedure:\n",
    "        1. A linear increase of the learning rate for a specified number of warmup epochs up to the base lr\n",
    "        2. A cosine decay of the learning rate throughout the remaining epochs\n",
    "    Args:\n",
    "        num_epochs: Number of epochs to train for.\n",
    "        steps_per_epoch: Number of steps per epoch.\n",
    "        base_lr: Base learning rate.\n",
    "        warmup_epochs: Number of epochs for warmup.\n",
    "    Returns:\n",
    "        learning_rate_fn: A function that takes the current step and returns the current learning rate.\n",
    "            It has the signature learning_rate_fn(step: int) -> lr.\n",
    "    \"\"\"\n",
    "    # Create the learning rate function implementing the procedure documented in the docstring\n",
    "    # Hint: use the following optax functions:\n",
    "    # optax.linear_schedule, optax.cosine_decay_schedule, optax.join_schedules\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return learning_rate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f92ac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a02961452e4a7efee1e2033981b061e6",
     "grade": false,
     "grade_id": "cell-f927532a22a5d3ed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_train_states(\n",
    "    rng: random.KeyArray, learning_rate_fn: Callable, weight_decay: float = 0.0\n",
    ") -> Dict[str, TrainState]:\n",
    "    \"\"\"\n",
    "    Initialize the train states of the two neural networks.\n",
    "    Args:\n",
    "        rng: PRNG key for pseudo-random number generation.\n",
    "        learning_rate_fn: A function that takes the current step and returns the current learning rate.\n",
    "            It has the signature learning_rate_fn(step: int) -> lr.\n",
    "        weight_decay: Weight decay of the Adam optimizer for training the neural networks.\n",
    "    Returns:\n",
    "        states: Dictionary containing the current states of the training of the two neural networks.\n",
    "            Entries of dictionary:\n",
    "                - MassMatrixNN: TrainState of the mass matrix neural network\n",
    "                - PotentialEnergyNN: TrainState of the potential energy neural network\n",
    "    \"\"\"\n",
    "    # initialize the neural network objects\n",
    "    mass_matrix_nn = MassMatrixNN()\n",
    "    potential_energy_nn = PotentialEnergyNN()\n",
    "\n",
    "    # initialize parameters of the neural networks by passing a dummy input through the network\n",
    "    # Hint: pass the `rng` and a dummy input to the `init` method of the neural network object\n",
    "    mass_matrix_nn_params = FrozenDict()\n",
    "    potential_energy_nn_params = FrozenDict()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # initialize the Adam with weight decay optimizer for both neural networks\n",
    "    # Hint: use optax.adamw\n",
    "    mass_matrix_nn_tx, potential_energy_nn_tx = None, None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # create the TrainState object for both neural networks\n",
    "    mass_matrix_nn_train_state = TrainState.create(\n",
    "        apply_fn=mass_matrix_nn.apply,\n",
    "        params=mass_matrix_nn_params,\n",
    "        tx=mass_matrix_nn_tx,\n",
    "    )\n",
    "    potential_energy_nn_train_state = TrainState.create(\n",
    "        apply_fn=potential_energy_nn.apply,\n",
    "        params=potential_energy_nn_params,\n",
    "        tx=potential_energy_nn_tx,\n",
    "    )\n",
    "\n",
    "    # save the TrainState objects into a dictionary, which we can pass to the various training functions\n",
    "    states = {\n",
    "        \"MassMatrixNN\": mass_matrix_nn_train_state,\n",
    "        \"PotentialEnergyNN\": potential_energy_nn_train_state,\n",
    "    }\n",
    "\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da9e5f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4c74294dfa08b8a579823d4e0c6f968",
     "grade": false,
     "grade_id": "cell-a44cea11d5d915cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def mse_loss_fn(pred: jnp.ndarray, target: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) loss function to train the neural network.\n",
    "    Args:\n",
    "        pred: Predictions of the neural network.\n",
    "        target: Targets (i.e. labels) for the supervised training.\n",
    "    Returns:\n",
    "        loss: Mean squared error (MSE) as array of shape () between the predicted and target values.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0e0e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c584d8bf2d2a102fe270a18a0f21f07d",
     "grade": false,
     "grade_id": "cell-a36336482f64d854",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_metrics(\n",
    "    batch: Dict[str, jnp.ndarray], preds: Dict[str, jnp.ndarray]\n",
    ") -> Dict[str, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes the metrics of the current batch measuring the performance of the dynamics prediction.\n",
    "    Args:\n",
    "        batch: dictionary of batch data (i.e. inputs and targets)\n",
    "        preds: dictionary of batch predictions (i.e. outputs of the neural network) with keys:\n",
    "            - th_next_pred: predicted next link angles of the double pendulum. [rad/s]\n",
    "            - th_d_next_pred: predicted next link angular velocities of the double pendulum. [rad/s^2]\n",
    "    Returns:\n",
    "        metrics: Dictionary of metrics\n",
    "    \"\"\"\n",
    "    error_th = normalize_link_angles(preds[\"th_next_ss\"] - batch[\"th_next_ss\"])\n",
    "    metrics = {\n",
    "        \"rmse_th_next\": jnp.sqrt(jnp.mean(jnp.square(error_th))),\n",
    "        \"rmse_th_d_next\": jnp.sqrt(\n",
    "            mse_loss_fn(preds[\"th_d_next_ss\"], batch[\"th_d_next_ss\"])\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f4e146",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2619466cd5eba6cf4ad9e634e3db9af",
     "grade": false,
     "grade_id": "cell-f827fa58e073c7f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Vectorize the discrete forward dynamics of the LNN\n",
    "# the discrete forward dynamics for a single state are available through the function `discrete_forward_dynamics`\n",
    "# It should have the following signature\n",
    "#   discrete_forward_dynamics_vmapped(\n",
    "#       mass_matrix_nn_params,\n",
    "#       potential_energy_nn_params,\n",
    "#       dt_ss,\n",
    "#       th_curr_ss,\n",
    "#       th_d_curr_ss,\n",
    "#       tau_ss,\n",
    "#   ) -> th_next_pred_ss, th_d_next_pred_ss, th_dd_ss\n",
    "# where `dt_ss` has the shape (N, ), `th_curr_ss` has the shape (N, 2), `th_d_curr_ss` has the shape (N, 2),\n",
    "# `tau_ss` has the shape (N, 2), `th_next_pred_ss` has the shape (N, 2), `th_d_next_pred_ss` has the shape (N, 2)\n",
    "# `th_dd_ss` has the shape (N, 2). N is the number of samples in the batch.\n",
    "\n",
    "discrete_forward_dynamics_vmapped = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed752cb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e84a275c6d713e8e5eefffa8de65737",
     "grade": false,
     "grade_id": "cell-fb21bea2fdef56a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@partial(\n",
    "    jit,\n",
    "    static_argnums=2,\n",
    "    static_argnames=\"learning_rate_fn\",\n",
    ")\n",
    "def train_step(\n",
    "    states: Dict[str, TrainState],\n",
    "    batch: Dict[str, jnp.ndarray],\n",
    "    learning_rate_fn: Callable,\n",
    ") -> Tuple[Dict[str, TrainState], Dict[str, jnp.ndarray]]:\n",
    "    \"\"\"\n",
    "    Trains the neural network for one step.\n",
    "    Args:\n",
    "        states: Dictionary containing the current states of the training of the two neural networks\n",
    "            Entries of dictionary:\n",
    "                - MassMatrixNN: TrainState of the mass matrix neural network\n",
    "                - PotentialEnergyNN: TrainState of the potential energy neural network\n",
    "        batch: dictionary of batch data\n",
    "        learning_rate_fn: A function that takes the current step and returns the current learning rate.\n",
    "            It has the signature learning_rate_fn(step: int) -> lr.\n",
    "    Returns:\n",
    "        states: Dictionary of updated training states\n",
    "            Entries of dictionary:\n",
    "                - MassMatrixNN: TrainState of the mass matrix neural network\n",
    "                - PotentialEnergyNN: TrainState of the potential energy neural network\n",
    "        metrics: Dictionary of training metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_fn(\n",
    "        mass_matrix_nn_params: Dict, potential_energy_nn_params: Dict\n",
    "    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n",
    "        \"\"\"\n",
    "        Loss function to train the neural network.\n",
    "        Args:\n",
    "            mass_matrix_nn_params: Parameters of the mass matrix neural network.\n",
    "            potential_energy_nn_params: Parameters of the potential energy neural network.\n",
    "        Returns:\n",
    "            loss: Mean squared error between the predicted and target link angular velocities at the next timestep.\n",
    "            preds: Dictionary of batch predictions (i.e. outputs of the neural network) with keys:\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the estimated link angles and velocities\n",
    "        # save the batch of predicted link angles, velocities at the next timestep and the link angular\n",
    "        # accelerations into the variables `th_next_pred_ss`, `th_d_next_pred_ss`, and `th_dd_pred_ss`\n",
    "        # Hint: use `discrete_forward_dynamics_vmapped`\n",
    "        _th_next_pred_ss = jnp.zeros_like(batch[\"th_curr_ss\"])\n",
    "        _th_d_next_pred_ss = jnp.zeros_like(batch[\"th_d_curr_ss\"])\n",
    "        _th_dd_pred_ss = jnp.zeros_like(batch[\"th_curr_ss\"])\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Compute the MSE loss on the predicted link angular velocities at the next timestep\n",
    "        _loss = jnp.array(0.0)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # write the predictions into a dictionary\n",
    "        _preds = {\n",
    "            \"th_next_ss\": _th_next_pred_ss,\n",
    "            \"th_d_next_ss\": _th_d_next_pred_ss,\n",
    "            \"th_dd_ss\": _th_dd_pred_ss,\n",
    "        }\n",
    "\n",
    "        return _loss, _preds\n",
    "\n",
    "    # compute loss and gradients with respect to the parameters of the two neural networks\n",
    "    # save the loss to `loss`, the dictionary with predictions to `preds`\n",
    "    # save the gradients to `grad_mass_matrix_nn` and `grad_potential_energy_nn`\n",
    "    # Hint: look at the `TrainState` source code to find-out how to access the NN params from the\n",
    "    # TrainState object.\n",
    "    # https://flax.readthedocs.io/en/latest/_modules/flax/training/train_state.html\n",
    "    # Hint: consider using `jax.value_and_grad` to get the loss and its gradient with respect to the NN parameters\n",
    "    loss = jnp.array(0.0)\n",
    "    grad_mass_matrix_nn, grad_potential_energy_nn = None, None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # optimize the mass matrix neural network parameters with gradient descent\n",
    "    states[\"MassMatrixNN\"] = states[\"MassMatrixNN\"].apply_gradients(\n",
    "        grads=grad_mass_matrix_nn\n",
    "    )\n",
    "    # optimize the potential energy neural network parameters with gradient descent\n",
    "    states[\"PotentialEnergyNN\"] = states[\"PotentialEnergyNN\"].apply_gradients(\n",
    "        grads=grad_potential_energy_nn\n",
    "    )\n",
    "\n",
    "    # compute metrics\n",
    "    metrics = compute_metrics(batch, preds)\n",
    "    metrics[\"loss\"] = loss\n",
    "    # save the currently active learning rates to the `metrics` dictionary\n",
    "    metrics[\"lr_mass_matrix_nn\"] = learning_rate_fn(states[\"MassMatrixNN\"].step)\n",
    "    metrics[\"lr_potential_energy_nn\"] = learning_rate_fn(\n",
    "        states[\"PotentialEnergyNN\"].step\n",
    "    )\n",
    "\n",
    "    return states, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e0c01",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9e1120830311c039d9e5e8410cb9f90",
     "grade": false,
     "grade_id": "cell-9f7acb72550a6621",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def eval_step(\n",
    "    states: Dict[str, TrainState], batch: Dict[str, jnp.ndarray]\n",
    ") -> Dict[str, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    One validation step of the neural networks.\n",
    "    Args:\n",
    "        states: Dictionary containing the current states of the training of the two neural networks\n",
    "            Entries of dictionary:\n",
    "                - MassMatrixNN: TrainState of the mass matrix neural network\n",
    "                - PotentialEnergyNN: TrainState of the potential energy neural network\n",
    "        batch: dictionary of batch data\n",
    "    Returns:\n",
    "        metrics: Dictionary of validation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the estimated link angles and velocities\n",
    "    # save the batch of predicted link angles, velocities at the next timestep and the link angular acceleration\n",
    "    # into the variables `th_next_pred_ss`, `th_d_next_pred_ss`, and `th_dd_pred_ss`\n",
    "    # Hint: use `discrete_forward_dynamics_vmapped`\n",
    "    # Hint: look at the `TrainState` source code to find-out how to access the NN params from the TrainState object\n",
    "    # https://flax.readthedocs.io/en/latest/_modules/flax/training/train_state.html\n",
    "    th_next_pred_ss = jnp.zeros_like(batch[\"th_curr_ss\"])\n",
    "    th_d_next_pred_ss = jnp.zeros_like(batch[\"th_d_curr_ss\"])\n",
    "    th_dd_pred_ss = jnp.zeros_like(batch[\"th_curr_ss\"])\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # write the predictions into a dictionary\n",
    "    preds = {\n",
    "        \"th_next_ss\": th_next_pred_ss,\n",
    "        \"th_d_next_ss\": th_d_next_pred_ss,\n",
    "    }\n",
    "\n",
    "    # Compute the MSE loss on the predicted link angular velocities at the next timestep\n",
    "    loss = jnp.array(0.0)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # compute metrics\n",
    "    metrics = compute_metrics(batch, preds)\n",
    "    metrics[\"loss\"] = loss\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a50f6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e7b7f6de65ac680740d799a4b4dc8d0",
     "grade": false,
     "grade_id": "cell-c4d11ef842a8355e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    states: Dict[str, TrainState],\n",
    "    train_ds: Dict[str, jnp.ndarray],\n",
    "    batch_size: int,\n",
    "    epoch: int,\n",
    "    learning_rate_fn: Callable,\n",
    "    rng: random.KeyArray,\n",
    ") -> Tuple[Dict[str, TrainState], float, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Train for a single epoch.\n",
    "    Args:\n",
    "        states: Dictionary containing the current states of the training of the two neural networks.\n",
    "            Entries of dictionary:\n",
    "                - MassMatrixNN: TrainState of the mass matrix neural network\n",
    "                - PotentialEnergyNN: TrainState of the potential energy neural network\n",
    "        train_ds: Dictionary containing the training dataset.\n",
    "        batch_size: Batch size of training loop.\n",
    "        epoch: Index of current epoch.\n",
    "        learning_rate_fn: A function that takes the current step and returns the current learning rate.\n",
    "            It has the signature learning_rate_fn(step: int) -> lr.\n",
    "        rng: PRNG key for pseudo-random number generation.\n",
    "    Returns:\n",
    "        states: Dictionary of updated training states.\n",
    "        train_loss: Training loss of the current epoch.\n",
    "        train_metrics: Dictionary of training metrics.\n",
    "    \"\"\"\n",
    "    train_ds_size = int(train_ds[\"th_curr_ss\"].shape[0])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, train_ds_size)  # get a randomized index array\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape(\n",
    "        (steps_per_epoch, batch_size)\n",
    "    )  # index array, where each row is a batch\n",
    "    batch_metrics = []\n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "        states, metrics = train_step(states, batch, learning_rate_fn)\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    # compute mean of metrics across each batch in epoch.\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean(jnp.array([metrics[k] for metrics in batch_metrics_np])).item()\n",
    "        for k in batch_metrics_np[0]\n",
    "    }  # jnp.mean does not work on lists\n",
    "\n",
    "    return states, epoch_metrics_np[\"loss\"], epoch_metrics_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda269e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22abd3c06500056987443f1a19c94bc2",
     "grade": false,
     "grade_id": "cell-efc01a9ee823b8f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    states: Dict[str, TrainState],\n",
    "    val_ds: Dict[str, jnp.ndarray],\n",
    ") -> Tuple[float, Dict[str, jnp.ndarray]]:\n",
    "    \"\"\"\n",
    "    Validate the model on the validation dataset.\n",
    "    Args:\n",
    "        states: Dictionary containing the current states of the training of the two neural networks.\n",
    "            Entries of dictionary:\n",
    "                - MassMatrixNN: TrainState of the mass matrix neural network\n",
    "                - PotentialEnergyNN: TrainState of the potential energy neural network\n",
    "        val_ds: Dictionary containing the validation dataset.\n",
    "    Returns:\n",
    "        val_loss: Validation loss.\n",
    "        val_metrics: Dictionary of metrics.\n",
    "    \"\"\"\n",
    "    val_metrics = eval_step(states, val_ds)\n",
    "    val_metrics = jax.device_get(val_metrics)\n",
    "    val_metrics = jax.tree_util.tree_map(\n",
    "        lambda x: x.item(), val_metrics\n",
    "    )  # map the function over all leaves in metrics\n",
    "\n",
    "    return val_metrics[\"loss\"], val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae27fe1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbb319ae575d97f3066840fcca4d65fa",
     "grade": false,
     "grade_id": "cell-bc083ab3691fd3ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def run_lnn_training(\n",
    "    rng: random.PRNGKey,\n",
    "    train_ds: Dict[str, jnp.ndarray],\n",
    "    val_ds: Dict[str, jnp.ndarray],\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    base_lr: float,\n",
    "    warmup_epochs: int = 0,\n",
    "    weight_decay: float = 0.0,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[\n",
    "    jnp.ndarray,\n",
    "    List[Dict[str, jnp.ndarray]],\n",
    "    List[Dict[str, jnp.ndarray]],\n",
    "    List[Dict[str, TrainState]],\n",
    "]:\n",
    "    \"\"\"\n",
    "    Run the training loop.\n",
    "    Args:\n",
    "        rng: PRNG key for pseudo-random number generation.\n",
    "        train_ds: Dictionary of jax arrays containing the training dataset.\n",
    "        val_ds: Dictionary of jax arrays containing the validation dataset.\n",
    "        num_epochs: Number of epochs to train for.\n",
    "        batch_size: The size of a minibatch (i.e. number of samples in a batch).\n",
    "        base_lr: Base learning rate (after warmup and before decay).\n",
    "        warmup_epochs: Number of epochs for warmup.\n",
    "        weight_decay: Weight decay.\n",
    "        verbose: If True, print the training progress.\n",
    "    Returns:\n",
    "        val_loss_history: Array of validation losses for each epoch.\n",
    "        val_metrics_history: List of dictionaries containing the validation metrics for each epoch.\n",
    "        train_states_history: List of dictionaries containing the training states for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # number of training samples\n",
    "    num_train_samples = len(train_ds[\"th_curr_ss\"])\n",
    "\n",
    "    # initialize the learning rate scheduler\n",
    "    learning_rate_fn = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # split of PRNG keys\n",
    "    # the 1st is used for training,\n",
    "    # the 2nd to initialize the neural network weights.\n",
    "    rng, init_train_states_rng = jax.random.split(rng, 2)\n",
    "\n",
    "    # initialize the train states\n",
    "    states = initialize_train_states(\n",
    "        init_train_states_rng, learning_rate_fn, weight_decay\n",
    "    )\n",
    "\n",
    "    # initialize the lists for the training history\n",
    "    val_loss_history = []  # list with validation losses\n",
    "    train_metrics_history = []  # list with train metric dictionaries\n",
    "    val_metrics_history = []  # list with validation metric dictionaries\n",
    "    states_history = []  # list with dictionaries of model states\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Training the Lagrangian neural network for {num_epochs} epochs...\")\n",
    "\n",
    "    for epoch in (pbar := tqdm(range(1, num_epochs + 1))):\n",
    "        # Split the `rng` PRNG key into two new keys\n",
    "        # use the 1st PRNG to update the `rng` variable\n",
    "        # store the 2nd PRNG key in the variable `epoch_rng`\n",
    "        epoch_rng = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Run the training for the current epoch\n",
    "        # Use the `epoch_rng` to randomly shuffle the batches\n",
    "        train_loss, train_metrics = jnp.array(0.0), {}\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Evaluate the current set of neural network parameters on the validation set\n",
    "        val_loss, val_metrics = jnp.array(0.0), {}\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Save the model parameters and the validation loss for the current epoch\n",
    "        val_loss_history.append(val_loss)\n",
    "        train_metrics_history.append(train_metrics)\n",
    "        val_metrics_history.append(val_metrics)\n",
    "        states_history.append(states)\n",
    "\n",
    "        if verbose:\n",
    "            pbar.set_description(\n",
    "                \"Epoch: %d, lr: %.6f, train loss: %.7f, val loss: %.7f\"\n",
    "                % (epoch, train_metrics[\"lr_mass_matrix_nn\"], train_loss, val_loss)\n",
    "            )\n",
    "\n",
    "    # array of shape (num_epochs, ) with the validation losses of each epoch\n",
    "    val_loss_history = jnp.array(val_loss_history)\n",
    "\n",
    "    return val_loss_history, train_metrics_history, val_metrics_history, states_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9da02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
