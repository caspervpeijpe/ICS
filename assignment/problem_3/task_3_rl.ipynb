{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac408b5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a96d02d6346fa7399cf6da0111ce0937",
     "grade": false,
     "grade_id": "cell-b00828259c8e42e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# RO47019: Intelligent Control Systems Practical Assignment\n",
    "* Period: 2022-2023, Q3\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/500969\n",
    "* Instructor: Cosimo Della Santina (C.DellaSantina@tudelft.nl)\n",
    "* Teaching assistant: Ruben Martin Rodriguez (R.MartinRodriguez@student.tudelft.nl)\n",
    "* (c) TU Delft, 2023\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Remove `raise NotImplementedError()` afterwards. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in your names, student numbers, netID, and emails below.\n",
    "STUDENT_1_NAME = \"\"\n",
    "STUDENT_1_STUDENT_NUMBER = \"\"\n",
    "STUDENT_1_NETID = \"\"\n",
    "STUDENT_1_EMAIL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba32571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "042927213b84aa368aa3ea72caa4cb60",
     "grade": true,
     "grade_id": "cell-9f148ec62e0de49c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert STUDENT_1_NAME != \"\"\n",
    "assert STUDENT_1_STUDENT_NUMBER != \"\"\n",
    "assert STUDENT_1_NETID != \"\"\n",
    "assert STUDENT_1_EMAIL != \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af317a94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e76ef40fcc3f08a0484661497162a1a9",
     "grade": false,
     "grade_id": "cell-4ea391677951116c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions, and do *not* copy solutions from others. By submitting your solutions, you claim that you alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled Q&A hours to ask the TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for youthat you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c956945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b714b090",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a69954ef588b900dc5f402d547a0f65",
     "grade": false,
     "grade_id": "cell-157f6cf278d2964c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 3 - Reinforcement Learning (45p)\n",
    "\n",
    "**Author:** Dr. Zlatan Ajanović (Z.Ajanovic@tudelft.nl)\n",
    "\n",
    "This exercise focuses on learning control policy without having need for a model, model-free RL. For this purpose, we use simplified version of the robot from problem 2 obtained by removing the second link (i.e., standard pendulum). \n",
    "\n",
    "\n",
    "We want to directly learn a control policy $\\pi(x)$ for rotating the pendulum to an angle $\\theta = \\frac{\\pi}{2}$ (move it in the upright position), only by interacting with the system. As currently stated, this is a simple task that can be solved by any model-free PD controller (e.g., Task 2.1). To have more fun, we make the problem more challenging by limiting the maximum torque $\\tau_\\mathrm{max}$ to a value preventing the trivial solution. Even if plenty of analytical solutions exist in the literature, it is quite interesting to see if RL can deal with such a nontrivial control problem without any model. By answering the theoretical questions and implementing their solutions you will construct a temporal difference reinforcement learning solution using the tabular SARSA algorithm.\n",
    "\n",
    "Analogue to problems 1 & 2, we define the position of the pendulum as the counter-clockwise rotation with respect to the horizontal x-axis, which points towards the right. The y-axis is pointing upwards and gravity is pointing downwards.\n",
    "\n",
    "Most of the theory needed to answer the questions in this assignment can be found in the book ''Reinforcement Learning: an Introduction'' by Sutton and Barto (S\\&B), Chapters 1, 2, 3, 4 and 6. An online version of this book can be found here: [RL book](http://incompleteideas.net/book/the-book.html). You can also look at the lecture slides.\n",
    "\n",
    "\n",
    "The complete code for this exercise is available in this Jupyter notebook (except the pendulum model and some libraries). First, you will implement different components of the RL solution, test and analyze them, and then integrate all the components in the RL framework. Finally, you will analyze RL framework performance and sensitivity to different design decisions. For each of the tasks, you need to implement some part of the code in that section. After that, you can use the test section to visualize and check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b691530",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd13908b5df6ee2dde94f686050649ab",
     "grade": false,
     "grade_id": "cell-eff725a3b7675ed2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Reloads the python files outside of this notebook automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import all Python modules\n",
    "from distutils.util import strtobool\n",
    "from functools import partial\n",
    "from IPython.display import display, HTML  # For animations in the notebook\n",
    "from jax.config import config as jax_config\n",
    "\n",
    "jax_config.update(\"jax_platform_name\", \"cpu\")  # set default device to 'cpu'\n",
    "jax_config.update(\"jax_enable_x64\", True)  # double precision\n",
    "from jax import numpy as jnp\n",
    "from jax import Array, jit, lax, random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # progress bar\n",
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "from jax_double_pendulum.analysis import *\n",
    "from jax_double_pendulum.motion_planning import (\n",
    "    generate_ellipse_trajectory,\n",
    "    ELLIPSE_PARAMS,\n",
    ")\n",
    "from jax_double_pendulum.robot_simulation import simulate_robot\n",
    "from jax_double_pendulum.visualization import animate_robot\n",
    "\n",
    "from analysis import (\n",
    "    plot_configuration_space_evolution,\n",
    "    plot_operational_space_evolution,\n",
    ")\n",
    "\n",
    "# define boolean to check if the notebook is run for the purposes of autograding\n",
    "AUTOGRADING = strtobool(os.environ.get(\"AUTOGRADING\", \"false\"))\n",
    "\n",
    "# create folder for outputs\n",
    "outputs_dir = Path(\"outputs\")\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a9d4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbfa5afc83099699a3f435337ebd3075",
     "grade": false,
     "grade_id": "cell-e7c62216b069d682",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.1 Familiarizing with the code (0p)\n",
    "\n",
    "Read the complete notebook. \n",
    "\n",
    "Think about these questions:\n",
    "\n",
    "- How many simulation steps are executed in a trial?\n",
    "- What are all the components of the framework?\n",
    "- How are they related and which ones depend on each other?\n",
    "- Compare the structure of the learning loop to SARSA in [RL Book](http://incompleteideas.net/book/ebook/node64.html), Figure 6.9 from the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f3211",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2090321526ee2f55877c9a2688df3a05",
     "grade": false,
     "grade_id": "cell-1139e1cfdada7ff4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.2 Setting the learning parameters (3p)\n",
    "\n",
    "Look at the `SARSA_PARAMS` and set the random action rate `epsilon` to $0.1$, and the learning rate `alpha` to $0.15$ and appropriate discount factor `gamma`.\n",
    "\n",
    "\n",
    "Learning is faster with higher learning rates. Why would we want to keep it low anyway? \n",
    "\n",
    "\n",
    "Set the action discretization to $15$ actions. Set the number of episodes to $8000$. Set the position discretization (i.e. `pos_states`) to $51$. Set the velocity discretization (i.e. `vel_states`) to $51$. Set the maximum pendulum torque to $5$ Nm.\n",
    "\n",
    "\n",
    "Run the section to make sure that you didn't make any obvious mistakes.\n",
    "\n",
    "These are not fixed parameters and you will probably spend some time tuning them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87674422",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3db839eedfaf5c1abda39bb809d09281",
     "grade": true,
     "grade_id": "task-3-2",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# SARSA learning params\n",
    "# hint - use dictionary keys: epsilon, gamma, alpha, pos_states, vel_states, actions, episodes, maxtorque\n",
    "\n",
    "SARSA_PARAMS = {\n",
    "    \"explore\": True,  # flag for choosing greedy only or explore\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567b9d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8c0227003649a995f2143e095b4edfc",
     "grade": false,
     "grade_id": "cell-89981231dc4a735b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.3 Initialize Q table (2p)\n",
    "\n",
    "The initial values in your Q table can be very important for the exploration behavior, and there are therefore\n",
    "many ways of initializing them (see S&B, Section 2.7). This is done in the init_Q(.) function.\n",
    "- Pick a method and give a short argumentation for your choice.\n",
    "- Implement your choice. The Q table should be of size N × M × O, where N is the number of\n",
    "position states, M is the number of velocity states, and O is the number of actions.\n",
    "\n",
    "Run the test section to find obvious mistakes. The test section should visualize also the first layer of the Q\n",
    "table. Run this section multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88063111",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc4a2e00bfd9bca59f55c86535be69da",
     "grade": true,
     "grade_id": "task-3-3-1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_Q(par: dict) -> Array:\n",
    "    \"\"\"\n",
    "    Initialize the Q table.\n",
    "\n",
    "    Args:\n",
    "        par: dictionary of SARSA learning parameters\n",
    "\n",
    "    Returns:\n",
    "        Q: Initial Q table of shape (N, M, O)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Initialize the Q table.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40787290",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b65c23c1f53629fa5501980eb44304ba",
     "grade": false,
     "grade_id": "cell-1821c291b9d95b53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Add your argumentation here (max 50 words):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79c1e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2535f315139a15fc6e9e259df155190",
     "grade": false,
     "grade_id": "task-3-3-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3777308",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8658a65902e81953919ae9b035b7356b",
     "grade": false,
     "grade_id": "cell-b288a83ff5c1d9a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "def plot_theta_theta_d(data: Array, par: dict, suptitle: str = None):\n",
    "    \"\"\"\n",
    "    Plot a Value function and optionally also a policy function over the discretized state space\n",
    "    Args:\n",
    "        data: array of shape (N, M, O) for plotting Q table or of shape (N, M) for plotting just value function\n",
    "        par: dictionary of RL parameters\n",
    "        suptitle: optional title / heading of figure\n",
    "    \"\"\"\n",
    "    n_plot = 20\n",
    "    scale = 1.0\n",
    "\n",
    "    xx = jnp.linspace(0, par[\"pos_states\"], num=par[\"pos_states\"] + 1)\n",
    "    xy = jnp.linspace(0, par[\"vel_states\"], num=par[\"vel_states\"] + 1)\n",
    "\n",
    "    def format_space_ax(ax):\n",
    "        x_tick_label = [r\"$\\pm\\pi$\", r\"$-\\pi/2$\", r\"$0$\", r\"$+\\pi/2$\", r\"$\\pm\\pi$\"]\n",
    "\n",
    "        ax.set_xlabel(r\"Angle\")\n",
    "        ax.set_ylabel(r\"Velocity\")\n",
    "        return ax\n",
    "\n",
    "    # case of ploting Q function\n",
    "    if jnp.shape(jnp.shape(data)) == (3,):\n",
    "        fig = plt.figure(figsize=(12, 4))\n",
    "        fig.subplots_adjust(\n",
    "            left=0.05, bottom=0.12, right=1.0, top=0.88, wspace=0.1, hspace=0.3\n",
    "        )\n",
    "\n",
    "        ax_val = format_space_ax(fig.add_subplot(1, 2, 1))\n",
    "        ax_pi = format_space_ax(fig.add_subplot(1, 2, 2))\n",
    "\n",
    "        ax_val.set_title(r\"$V(x)$\")\n",
    "        ax_pi.set_title(r\"$\\pi(x)$\")\n",
    "\n",
    "        V_mat = jnp.amax(data, axis=2)  # maximum along actions axis\n",
    "        a_mat = jnp.argmax(data, axis=2)  # maximum along actions axis\n",
    "\n",
    "        a_mat = gaussian_filter(a_mat, sigma=1)\n",
    "\n",
    "        cset = ax_pi.pcolormesh(\n",
    "            xx,\n",
    "            xy,\n",
    "            jnp.transpose(a_mat),\n",
    "            shading=\"flat\",\n",
    "            vmin=0,\n",
    "            vmax=par[\"actions\"],\n",
    "            cmap=\"seismic\",\n",
    "        )\n",
    "        plt.colorbar(cset, ax=ax_pi)\n",
    "\n",
    "    else:  # ploting other functions\n",
    "        fig = plt.figure(figsize=(6.4, 4.8))\n",
    "\n",
    "        ax_val = format_space_ax(fig.add_subplot(1, 1, 1))\n",
    "\n",
    "        V_mat = data\n",
    "\n",
    "    if suptitle is not None:\n",
    "        plt.suptitle(suptitle)\n",
    "\n",
    "    cset = ax_val.pcolormesh(\n",
    "        xx,\n",
    "        xy,\n",
    "        jnp.transpose(V_mat),\n",
    "        shading=\"flat\",\n",
    "        vmin=0,\n",
    "        vmax=jnp.abs(V_mat).max(),\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(cset, ax=ax_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003d39f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "487606b2a7c67b8aa45afb487ff11f09",
     "grade": false,
     "grade_id": "cell-6f23b811a8702153",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Visualize 1st layerSta\n",
    "\n",
    "Q = init_Q(\n",
    "    SARSA_PARAMS,\n",
    ")\n",
    "\n",
    "plot_theta_theta_d(Q, SARSA_PARAMS, suptitle=\"Initialized Q table\")\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(str(outputs_dir / \"task_3-3_initialize_Q_table.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcaa6d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2dff860f704e65392203cc37733e80d7",
     "grade": false,
     "grade_id": "cell-48a0cdfa80493407",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.4 Discretize state (5p)\n",
    "\n",
    "\n",
    "In Task 3.2, you determined the number of position and velocity states that your Q table can hold, and the\n",
    "number of actions the agent can choose from. The state discretization is done in the `discretize_state(.)`\n",
    "function.\n",
    "- Implement the position discretization. The input may be outside the interval $[-\\pi, \\pi]$ rad, so be\n",
    "sure to wrap the state around (hint: use the mod function). The resulting state must be in the range\n",
    "`[0, pos_states-1]`. Make sure to perform the mapping such as that $\\frac{\\pi}{2}$ rad (the “up” position) will be in the center of the discrete range.\n",
    "- Implement the velocity discretization. Even though we assume that the values will not exceed the\n",
    "range `[−5π, 5π] rad s−1`, they must be clipped to that range to avoid errors. The resulting state\n",
    "must be in the range `[0, vel_states-1]`. Make sure to perform the mapping such as that zero velocity will be in the center of the discrete range.\n",
    "- What would happen if we clip the velocity range too soon, say at `[−2π, 2π] rad/s`?\n",
    "\n",
    "Run the test section, and look at the plots of continuous vs. discretized position. Are they what you would expect?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86178ac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d17723c05959f41bf06a93d8585d972",
     "grade": true,
     "grade_id": "task-3-4",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def discretize_state(x: Array, par: Dict) -> Array:\n",
    "    \"\"\"\n",
    "    Discretize the state\n",
    "    Args:\n",
    "        x: state in the continuous domain. Float64 array of shape (2, )\n",
    "        par: Dictionary with parameters\n",
    "    Returns:\n",
    "        s: state in the discrete domain. Int64 array of shape (2, )\n",
    "    \"\"\"\n",
    "    # TODO: Discretize state.\n",
    "    # Note: s[0] should be pendulum position, s[1] velocity.\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return s.astype(jnp.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911ace5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7fbdbc485c2c3a01689fe8c96f2b2b9",
     "grade": false,
     "grade_id": "cell-fea70bba8545e5e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test and plot position discretization\n",
    "x0 = jnp.zeros((2,))\n",
    "pos_con = jnp.arange(-4 * jnp.pi, 4 * jnp.pi, step=0.05)\n",
    "pos_dis = jnp.zeros(pos_con.shape)\n",
    "for pp in range(len(pos_con)):\n",
    "    x0 = x0.at[0].set(pos_con[pp])\n",
    "    s = discretize_state(x0, SARSA_PARAMS)\n",
    "    pos_dis = pos_dis.at[pp].set(s[0])\n",
    "\n",
    "# test vertical pose\n",
    "s = discretize_state(jnp.array([jnp.pi / 2, 0]), SARSA_PARAMS)\n",
    "print(\"s:\", s)\n",
    "\n",
    "plt.plot(pos_con, pos_dis, label=\"mapping continous-discrete\")\n",
    "plt.plot(jnp.pi / 2, s[0], \"ro\", label=\"up pose\")\n",
    "\n",
    "plt.xlabel(r\"Continuous Angle [rad]\")\n",
    "plt.ylabel(\"Position discretization\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(str(outputs_dir / \"task_3-4_position_discretization.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54d5b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2e4453835a480d0239794e0f82ca9f7",
     "grade": false,
     "grade_id": "cell-4d3e4860646cce8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test and plot velocity discretization\n",
    "x0 = jnp.zeros((2,))\n",
    "vel_con = jnp.arange(-20, 20, 0.1)\n",
    "vel_dis = jnp.zeros(vel_con.shape)\n",
    "\n",
    "for vv in range(len(vel_con)):\n",
    "    x0 = x0.at[1].set(vel_con[vv])\n",
    "    s = discretize_state(x0, SARSA_PARAMS)\n",
    "    vel_dis = vel_dis.at[vv].set(s[1])\n",
    "\n",
    "plt.plot(vel_con, vel_dis, label=\"mapping continous-discrete\")\n",
    "\n",
    "plt.xlabel(r\"Continuous velocity [rad/s]\")\n",
    "plt.ylabel(\"Velocity discretization\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.savefig(str(outputs_dir / \"task_3-4_velocity_discretization.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7cc6af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9fe2a9a0569b7e63d9bf34d1a270547",
     "grade": false,
     "grade_id": "cell-0151afa79ba2be90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.5 Conversion of actions to torques (2p)\n",
    "\n",
    "Now you need to specify how the discrete actions are turned into torque values, in the `convert_action_to_torque(.)`\n",
    "function.\n",
    "- The allowable torque is in the range `[−maxtorque, maxtorque]`. Distribute the actions\n",
    "uniformly over this range. This means that zero torque will be in the middle of the range.\n",
    "\n",
    "Run the test section, and look at the plots of continuous vs. discretized position. Are they what you would\n",
    "expect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0382132",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c77dea5a474a7b96a8dea4dc2e75864",
     "grade": true,
     "grade_id": "task-3-5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Action execution\n",
    "@jit\n",
    "def convert_action_to_torque(a: Array, par: Dict) -> Array:\n",
    "    \"\"\"\n",
    "    Convert a discrete action to a continuous, saturated torque, which can be applied to the system\n",
    "    Args:\n",
    "        a: discrete action as array of shape ()\n",
    "        par: Dictionary with parameters\n",
    "    Returns:\n",
    "        tau: continuous torque as array of shape ()\n",
    "    \"\"\"\n",
    "    # TODO: Calculate the proper torque for action a.\n",
    "    # This cannot exceed par[\"maxtorque\"].\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3c26a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25939d37bee1ecb61b499728fadd87e8",
     "grade": false,
     "grade_id": "cell-3ba4dc30983493fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "torque_dis = jnp.zeros(SARSA_PARAMS[\"actions\"])\n",
    "\n",
    "for aa in range(SARSA_PARAMS[\"actions\"]):\n",
    "    torque = convert_action_to_torque(aa, SARSA_PARAMS)\n",
    "    torque_dis = torque_dis.at[aa].set(torque)\n",
    "\n",
    "print(\"Torques:\\n\", torque_dis)\n",
    "\n",
    "plt.plot(torque_dis, \"o\", label=\"sampled action\")\n",
    "\n",
    "plt.xlabel(r\"Discrete action\")\n",
    "plt.ylabel(\"Continous torque [Nm]\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.savefig(str(outputs_dir / \"task_3-5_action_to_torque.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd026d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9ecfaffaba317c83b414789de8c77a4",
     "grade": false,
     "grade_id": "cell-ca4b96f764c19108",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.6. Reward design (3p)\n",
    "\n",
    "Now you should determine the reward function, which is implemented in `observe_reward(.)`.\n",
    "- What is the simplest reward function that you can devise, given that we want the system to balance\n",
    "the pendulum at the top?\n",
    "- Implement `observe_reward(.)` function.\n",
    "- **Hint:** If you need to express conditions, use [`lax.select`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.select.html) instead of `if` clauses, as pythonic `if` conditions are not jittable.\n",
    "\n",
    "Run the test section, and verify in the plot that you have indeed implemented the reward function you\n",
    "wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4854e2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa641123ab52cc08fcf5102b936639a7",
     "grade": true,
     "grade_id": "task-3-6",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def observe_reward(a: Array, sP: Array, par: Dict) -> float:\n",
    "    \"\"\"\n",
    "    Distributes a reward for the action `a` resulting in state `sP`.\n",
    "    Remember that the goal is to swing-up the pendulum to a link angle of $\\frac{\\pi}{2}$.\n",
    "    Args:\n",
    "        a: discrete action as array of shape ()\n",
    "        sP: state resulting from taking the action `a` as array of shape (2, )\n",
    "    Returns:\n",
    "        r: reward for the state-action pair\n",
    "    \"\"\"\n",
    "    # TODO: Calculate the reward for taking action a, resulting in state sP.\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846152d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51b42280684eb9b8974b29e3b48b85cf",
     "grade": false,
     "grade_id": "cell-2b14ea906b4bf683",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "# Attention: ignore the figure titles! Only reusing the plot function for the Q.\n",
    "\n",
    "# initialize array with reward distribution\n",
    "r_dist = jnp.zeros(\n",
    "    (SARSA_PARAMS[\"pos_states\"], SARSA_PARAMS[\"vel_states\"], SARSA_PARAMS[\"actions\"])\n",
    ")\n",
    "\n",
    "pbar = tqdm(total=SARSA_PARAMS[\"pos_states\"] * SARSA_PARAMS[\"vel_states\"])\n",
    "for pp in range(SARSA_PARAMS[\"pos_states\"]):\n",
    "    for vv in range(SARSA_PARAMS[\"vel_states\"]):\n",
    "        for aa in range(SARSA_PARAMS[\"actions\"]):\n",
    "            # state\n",
    "            s = jnp.array([pp, vv])\n",
    "\n",
    "            # observe reward\n",
    "            r = observe_reward(aa, s, SARSA_PARAMS)\n",
    "\n",
    "            # insert reward for current state-action pair\n",
    "            r_dist = r_dist.at[pp, vv, aa].set(r)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "rm = jnp.mean(r_dist, axis=2)\n",
    "\n",
    "# plot the reward distribution\n",
    "plot_theta_theta_d(rm, SARSA_PARAMS, suptitle=\"Reward distribution\")\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(str(outputs_dir / \"task_3-6_reward_distribution.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0bf87e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e615b4c6d137636ae98c228ad903606e",
     "grade": true,
     "grade_id": "cell-e8f3c81c8c47143f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x0 = jnp.array([jnp.pi / 2, 0])\n",
    "s = discretize_state(x0, SARSA_PARAMS)\n",
    "r_t = observe_reward(aa, s, SARSA_PARAMS)\n",
    "\n",
    "x1 = jnp.array([0.9 * jnp.pi / 2, 0])\n",
    "s1 = discretize_state(x1, SARSA_PARAMS)\n",
    "\n",
    "r_1 = observe_reward(aa, s1, SARSA_PARAMS)\n",
    "\n",
    "assert r_t > r_1\n",
    "\n",
    "x1 = jnp.array([1.1 * jnp.pi / 2, 0])\n",
    "s1 = discretize_state(x1, SARSA_PARAMS)\n",
    "\n",
    "r_1 = observe_reward(aa, s1, SARSA_PARAMS)\n",
    "\n",
    "assert r_t > r_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b248fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c5b010e9ba35568f7f756f486ed5e77",
     "grade": false,
     "grade_id": "cell-9c3ab20e2038910c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Task 3.7. Episode termination (3p)\n",
    "\n",
    "You also need to implement a criterium for stopping the episode. While we could learn to continually\n",
    "balance the pendulum, in this exercise we will only learn to swing up into a balanced state, so-called\n",
    "episodic learning. The trial can therefore be ended when that goal state is reached.\n",
    "\n",
    "**Hint:** If you need to express conditions, use [`lax.select`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.select.html) instead of `if` clauses, as pythonic `if` conditions are not jittable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad94412",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9337b77862090d892050f8d3739f38f",
     "grade": true,
     "grade_id": "task-3-7",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def is_terminal(sP: Array, par: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Criterium for stopping the episode\n",
    "    Args:\n",
    "        sP: state as array of shape (2, )\n",
    "        par: Dictionary with parameters\n",
    "    Returns:\n",
    "        t: scalar indicating whether the episode should be stopped (1) or be continued (0)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Return 1 if state sP is terminal, 0 otherwise.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d55e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2069440499980f86bf04819c048776c0",
     "grade": false,
     "grade_id": "cell-d2b48fbc96ce8b7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "# Attention: ignore the figure titles! Only reusing the plot function for the Q.\n",
    "\n",
    "t_dist = jnp.zeros((SARSA_PARAMS[\"pos_states\"], SARSA_PARAMS[\"vel_states\"]))\n",
    "\n",
    "pbar = tqdm(total=SARSA_PARAMS[\"pos_states\"] * SARSA_PARAMS[\"vel_states\"])\n",
    "for pp in range(SARSA_PARAMS[\"pos_states\"]):\n",
    "    for vv in range(SARSA_PARAMS[\"vel_states\"]):\n",
    "        # state\n",
    "        s = jnp.array([pp, vv])\n",
    "\n",
    "        # evaluate the termination criterion for the current state\n",
    "        t = is_terminal(s, SARSA_PARAMS)\n",
    "\n",
    "        # store terminal condition\n",
    "        t_dist = t_dist.at[pp, vv].set(t)\n",
    "\n",
    "        pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "plot_theta_theta_d(t_dist, SARSA_PARAMS, suptitle=\"Termination criterion\")\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(str(outputs_dir / \"task_3-7_termination_criterion.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66471ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa7e058d2540ea3e535cfbd951513c7f",
     "grade": false,
     "grade_id": "cell-97903cae14539885",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.8. The policy selection (2p)\n",
    "\n",
    "It is time to implement the action selection rule in `evaluate_policy(.)` function. See S&B, Sections\n",
    "2.2 and 6.4.\n",
    "- Implement the greedy action selection algorithm.\n",
    "- Modify the chosen action according to the ε-greedy policy. Hint: use the rand and randint\n",
    "functions.\n",
    "- **Hint:** If you need to express conditions, use [`lax.select`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.select.html) instead of `if` clauses, as pythonic `if` conditions are not jittable.\n",
    "\n",
    "Run the test section to check for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba5e5e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9749e16a40cb59967da10d1c68592ee3",
     "grade": true,
     "grade_id": "task-3-8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def evaluate_policy(\n",
    "    Q: Array, s: Array, par: Dict, rng: random.KeyArray\n",
    ") -> Tuple[Array, random.KeyArray]:\n",
    "    \"\"\"\n",
    "    Evaluate the policy and choose a discrete action.\n",
    "    Args:\n",
    "        Q: Q table of shape (N, M, O)\n",
    "        s: state as array of shape (2, )\n",
    "        par: Dictionary with parameters\n",
    "        rng: PRNG key for pseudo-random number generation.\n",
    "    Returns:\n",
    "        a: discrete action as array of shape ()\n",
    "        rng: updated PRNG key for pseudo-random number generation.\n",
    "    \"\"\"\n",
    "    # Hint: use the functions random.uniform(.) and random.randint (.)\n",
    "\n",
    "    rng, rng_exploration, rng_action = random.split(rng, 3)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return a, rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e912058",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0bdeb40a2669b79920f2fbeaa9560ce",
     "grade": false,
     "grade_id": "cell-9dcfde220d81236c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "key = random.PRNGKey(758493)\n",
    "\n",
    "pbar = tqdm(\n",
    "    total=SARSA_PARAMS[\"pos_states\"]\n",
    "    * SARSA_PARAMS[\"vel_states\"]\n",
    "    * SARSA_PARAMS[\"actions\"]\n",
    ")\n",
    "for pp in range(SARSA_PARAMS[\"pos_states\"]):\n",
    "    for vv in range(SARSA_PARAMS[\"vel_states\"]):\n",
    "        # define state\n",
    "        s = jnp.array([pp, vv])\n",
    "\n",
    "        for aa in range(SARSA_PARAMS[\"actions\"]):\n",
    "            a, key = evaluate_policy(Q, s, SARSA_PARAMS, key)\n",
    "\n",
    "            if a < 0 or a > SARSA_PARAMS[\"actions\"]:\n",
    "                print(\"Action out of the bound\")\n",
    "\n",
    "            pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b2c12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12f21f0a510eddbf8d13a7715ceb4f1a",
     "grade": false,
     "grade_id": "cell-78bb42847d59a795",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test 2\n",
    "Q1 = Q\n",
    "\n",
    "key = random.PRNGKey(758493)\n",
    "\n",
    "opt_action = 2\n",
    "Q1 = Q1.at[5, 5, opt_action].set(100)\n",
    "count_greedy = 0\n",
    "count_random = 0\n",
    "test_count = 10000\n",
    "\n",
    "pbar = tqdm(total=test_count)\n",
    "for i in range(test_count):\n",
    "    a, key = evaluate_policy(Q1, jnp.array([5, 5]), SARSA_PARAMS, key)\n",
    "    if a == opt_action:\n",
    "        count_greedy = count_greedy + 1\n",
    "    else:\n",
    "        count_random = count_random + 1\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "count_greedy_ref = (\n",
    "    1 - SARSA_PARAMS[\"epsilon\"] + SARSA_PARAMS[\"epsilon\"] * 1 / SARSA_PARAMS[\"actions\"]\n",
    ")\n",
    "\n",
    "assert (\n",
    "    count_greedy / test_count > 0.9 * count_greedy_ref\n",
    "    and count_greedy / test_count < 1.1 * count_greedy_ref\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80bc7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e919401151e8fff06b4f4a96645c4ab3",
     "grade": false,
     "grade_id": "cell-79ecc59bff6508df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.9. Value update rule (4p)\n",
    "\n",
    "In this task, you should implement the SARSA update rule in the `update_Q(.)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f6b71",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8775a2a56076f839d9ca1a5d67cc00fa",
     "grade": true,
     "grade_id": "task-3-9",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def update_Q(\n",
    "    Q: Array, s: Array, a: Array, r: Array, sP: Array, aP: Array, par: Dict\n",
    ") -> Array:\n",
    "    \"\"\"\n",
    "    Update the Q-function after one learning iteration, e.g. using SARSA.\n",
    "    Args:\n",
    "        Q: Q table of shape (N, M, O)\n",
    "        s: discrete state as an array of shape (2, )\n",
    "        a: discrete action as an array of shape ()\n",
    "        r: reward as a float variable\n",
    "        sP: discrete state in the next step as an array of shape (2, )\n",
    "        ap: discrete action in the next step as an array of shape ()\n",
    "        par: dictionary with parameters\n",
    "    Returns:\n",
    "        Q: updated Q table of shape (N, M, O)\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f79ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51d09e46a329cbb9ad42649f874f6556",
     "grade": false,
     "grade_id": "cell-a9410eea1541ad3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Testing value update (note that policy plot is blured)\n",
    "\n",
    "# initial Q table\n",
    "Q_test = jnp.zeros(\n",
    "    (SARSA_PARAMS[\"pos_states\"], SARSA_PARAMS[\"vel_states\"], SARSA_PARAMS[\"actions\"])\n",
    ")\n",
    "\n",
    "# plot the initial Q table\n",
    "plot_theta_theta_d(Q_test, SARSA_PARAMS, suptitle=\"Q table before update\")\n",
    "plt.savefig(str(outputs_dir / \"task_3-9_Q_table_before_update.pdf\"))\n",
    "\n",
    "s = jnp.array([10, 3])\n",
    "sP = jnp.array([10, 2])\n",
    "\n",
    "# update the Q table\n",
    "Q_test = update_Q(Q=Q_test, s=s, a=2, r=100, sP=sP, aP=2, par=SARSA_PARAMS)\n",
    "\n",
    "# plot the updated Q table\n",
    "plot_theta_theta_d(Q_test, SARSA_PARAMS, suptitle=\"Q table after update\")\n",
    "# save the figure\n",
    "plt.savefig(str(outputs_dir / \"task_3-9_Q_table_after_update.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8819793",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9796c9d54e345e6d1412359f4074009d",
     "grade": false,
     "grade_id": "cell-b576d7f9497ef20e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.10. Make it work (12p)\n",
    "\n",
    "Finally, implement SARSA as described in [RL Book](http://incompleteideas.net/book/ebook/node64.html), Figure 6.9 and complete all the code of the learning loop (initializations of outer and inner loops, calculation of torque, learning and termination). Basically, you need to call all functions prepared in Tasks 3.3-3.9 in the right order. Also, make sure that the initial state is always slightly perturbed, i.e., that `sample_initial_state(.)` is used for initialization.\n",
    "\n",
    "After that, run the training section and see how your learning algorithm behaves! Run animation and check the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b0ea6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb5b1db2c722707c6421c9b486268cb3",
     "grade": false,
     "grade_id": "cell-e7907246a1fa17fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.10.1: Initialize pendulum (2p)\n",
    "\n",
    "Here you should implement the strategy to initialize the pendulum at the start of each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782beb4c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20946ca8f9b456358d8e4b49b09b928a",
     "grade": true,
     "grade_id": "task-3-10-1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def sample_initial_state(rng: random.KeyArray) -> Tuple[Array, random.KeyArray]:\n",
    "    \"\"\"\n",
    "    Randomly sample the initial state of the pendulum\n",
    "    Args:\n",
    "        rng: PRNG key for pseudo-random number generation.\n",
    "    Returns:\n",
    "        x0: initial state of the pendulum as an array of shape (2, )\n",
    "        rng: updated PRNG key for pseudo-random number generation.\n",
    "    \"\"\"\n",
    "    # TODO Set initial state for the experiment, cover the whole state-space\n",
    "    # Hint: make sure to split the PRNG into three new keys\n",
    "    #       the first key should be returned again by the function\n",
    "    #       the other two keys can be used for the random sampling of the initial state\n",
    "    # Hint: use random.uniform\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return x0, rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4d9b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d43760ef3c5819088df6848fff221229",
     "grade": false,
     "grade_id": "cell-d50678c7ed495356",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "test_size = 1000\n",
    "x0_dist = jnp.zeros([2, test_size])\n",
    "for i in (pbar := tqdm(range(test_size))):\n",
    "    x0, key = sample_initial_state(key)\n",
    "    x0_dist = x0_dist.at[:, i].set(x0)\n",
    "\n",
    "plt.hist(x0_dist[0, :], density=True, bins=30)  # density=False would make counts\n",
    "plt.suptitle(\"Histogram of initial link angle\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Angle\")\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(str(outputs_dir / \"task_3-10-1_histogram_of_initial_state.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a5b3c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de4d308cbc184094ea736fda3b1f59c7",
     "grade": false,
     "grade_id": "cell-dac023c329be4cc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Pendulum model\n",
    "\n",
    "As our simulator is implemented for a double pendulum, we have to simulate two links. However, we will choose the parameters of the 2nd link such that there are only neglible dynamical effects on the first link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a568eb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d445159b93a6e380161491eb96579e5f",
     "grade": false,
     "grade_id": "cell-52f225a96dcbcd07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ROBOT_PARAMS_SINGLE = dict(\n",
    "    l1=1,  # Length of link 1 [m]\n",
    "    lc1=1,  # Distance of CoM of link 1 from base [m]\n",
    "    m1=2,  # Mass of link 1 [kg]\n",
    "    j1=1 / 15 * 2 * 1**2,  # Inertia of link 1 [kg*m^2] about its CoM\n",
    "    l2=1e-2,  # Length of link 2 [m]\n",
    "    lc2=1e-2,  # Distance of CoM of link 2 from elbow [m]\n",
    "    m2=1e-2,  # Mass of link 2 [kg]\n",
    "    j2=1e-2,  # Inertia of link 2 [kg*m^2] about its CoM\n",
    "    g=9.81,  # Gravitational acceleration [m/s^2]\n",
    ")\n",
    "\n",
    "dt = 5e-2  # time step [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate pendulum\n",
    "\n",
    "# initial state of the single pendulum\n",
    "s_0 = jnp.array([jnp.pi / 4, 0.0])\n",
    "\n",
    "# convert single pendulum state to double state\n",
    "th_0 = jnp.array([s_0[0], 0.0])\n",
    "th_d_0 = jnp.array([s_0[1], 0.0])\n",
    "\n",
    "sim_duration = 10.0  # simulation duration [s]\n",
    "\n",
    "t_ts = dt * jnp.arange(int(sim_duration / dt))\n",
    "\n",
    "sim_ts = simulate_robot(\n",
    "    rp=ROBOT_PARAMS_SINGLE,\n",
    "    t_ts=t_ts,\n",
    "    th_0=th_0,\n",
    "    th_d_0=th_d_0,\n",
    "    tau_ext_ts=jnp.array([10.0, 0.0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ad102",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82bc8a4e0be9898e26955b5ab327cd59",
     "grade": false,
     "grade_id": "cell-949154096dd56013",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the evolution of the single pendulum in configuration space\n",
    "plot_configuration_space_evolution(\n",
    "    sim_ts,\n",
    "    filepath=str(\n",
    "        outputs_dir / \"task_3-10_autonomous_configuration_space_evolution.pdf\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd96a1d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa91d8ebc94747f3442663775a78a107",
     "grade": false,
     "grade_id": "cell-815e0450143d61b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the evolution of the single pendulum in operational space\n",
    "plot_operational_space_evolution(\n",
    "    sim_ts,\n",
    "    filepath=str(outputs_dir / \"task_3-10_autonomous_operational_space_evolution.pdf\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1f5cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e0aeec04fd11005224523ba8ef70ccf",
     "grade": false,
     "grade_id": "cell-b325c1822e26a60d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    from IPython.display import display, HTML  # For animations in the notebook\n",
    "\n",
    "    ani = animate_robot(\n",
    "        ROBOT_PARAMS_SINGLE,\n",
    "        sim_ts=sim_ts,\n",
    "        step_skip=2,\n",
    "        show=False,\n",
    "        filepath=str(outputs_dir / \"task_3-10_autonomous_robot.mp4\"),\n",
    "    )\n",
    "    display(HTML(ani.to_html5_video()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116030a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0f807c015de436726fe5b5bb5656d9e",
     "grade": false,
     "grade_id": "cell-b3b4966e00bf6a05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.10.2: Implement the RL step function (4p)\n",
    "\n",
    "In this section, you should implement one of the RL training.\n",
    "\n",
    "1. Take the action\n",
    "2. Simulate the step using the `discrete_forward_dynamics`\n",
    "3. Observe the reward of the state-action pair\n",
    "4. Sample a new action\n",
    "5. Update the Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c88deb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e0728029f04ceaae2e95ad25f177bd9",
     "grade": true,
     "grade_id": "task-3-10-2",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import double pendulum forward dynamics\n",
    "from jax_double_pendulum.dynamics import discrete_forward_dynamics\n",
    "\n",
    "\n",
    "@jit\n",
    "def run_rl_step(\n",
    "    Q: Array,\n",
    "    dt: float,\n",
    "    x: Array,\n",
    "    s: Array,\n",
    "    a: Array,\n",
    "    cum_r: float,\n",
    "    rp: Dict[str, Array],\n",
    "    par: Dict,\n",
    "    rng: random.KeyArray,\n",
    ") -> Tuple[Array, Array, Array, Array, float, random.KeyArray]:\n",
    "    \"\"\"\n",
    "    Run one RL step\n",
    "    Args:\n",
    "        Q: Q table of shape (N, M, O)\n",
    "        dt: time step of simulation [s]\n",
    "        x: state in the continuous domain as array of shape (2, )\n",
    "        s: discrete state as array of shape (2, )\n",
    "        a: discrete action as array of shape ()\n",
    "        cum_r: cumulative reward\n",
    "        rp: dictionary with robot parameters\n",
    "        par: dictionary with RL parameters\n",
    "        rng: PRNG key for pseudo-random number generation.\n",
    "    Returns:\n",
    "        Q: updated Q table of shape (N, M, O)\n",
    "        xP: next state in the continuous domain as array of shape (2, )\n",
    "        sP: next discrete state as array of shape (2, )\n",
    "        aP: next discrete action as array of shape ()\n",
    "        cum_r: updated cumulative reward\n",
    "        rng: updated PRNG key for pseudo-random number generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Keep track of cumulative reward\n",
    "    cum_r = cum_r + r\n",
    "\n",
    "    return Q, xP, sP, aP, cum_r, rng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17086051",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2d7f82ce10e0171968fb1d4207bd840",
     "grade": false,
     "grade_id": "cell-e1f4b6c7940f9d68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.10.3: Run the RL training (6p)\n",
    "\n",
    "In this section, you should implement the whole simulation by using the function for a single step. As the `run_rl_step` function is compiled using `jit` it should run faster than native Python. Each episode should contain the following steps:\n",
    "\n",
    "1. **Optional:** Decay the exploration parameter `epsilon`.\n",
    "2. Randomly initialize the state (i.e. position and velocity) of the pendulum.\n",
    "3. Evaluate the policy at the current state.\n",
    "4. Simulate the system for `num_steps`. At each step, execute `run_rl_step` and evaluate the terminal condition to decide if the episode should be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a82369",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05b6213d829c860a33d709900857fdc6",
     "grade": true,
     "grade_id": "task-3-10-3",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "# simulation parameters\n",
    "sim_duration = 50.0  # [s]\n",
    "\n",
    "# define the number of steps\n",
    "num_steps = int(sim_duration / dt)\n",
    "t_ts = dt * jnp.arange(num_steps)\n",
    "\n",
    "# initialize the PRNG key for random number generation\n",
    "key = random.PRNGKey(548815)\n",
    "\n",
    "# copy the `SARSA_PARAMS`\n",
    "par = SARSA_PARAMS.copy()\n",
    "\n",
    "# initialize Q table\n",
    "Q = init_Q(SARSA_PARAMS)\n",
    "\n",
    "# Initialize bookkeeping (for plotting only)\n",
    "ra = jnp.zeros((par[\"episodes\"],))  # cumulative reward\n",
    "tta = jnp.zeros((par[\"episodes\"],))  # duration of each episode\n",
    "te = 0\n",
    "\n",
    "# Outer loop: episodes\n",
    "for ii in range(par[\"episodes\"]):\n",
    "    # Update plot every 100 episodes\n",
    "    if jnp.mod(ii, 100) == 0:\n",
    "        display.clear_output(wait=True)  # for updating plot\n",
    "        display.display(plt.gcf())  # for updating plot\n",
    "        plot_theta_theta_d(Q, par, suptitle=\"Episode %i\" % ii)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Keep track of cumulative reward and time of each episode\n",
    "    ra = ra.at[ii].set(cum_r)\n",
    "    tta = tta.at[ii].set(tt * dt)\n",
    "\n",
    "# save the last plot\n",
    "plt.savefig(\n",
    "    str(outputs_dir / \"task_3-10-3_value_and_policy_function_after_training.pdf\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c200b3f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69142615f2184a5a5270013ae6234c48",
     "grade": false,
     "grade_id": "cell-113bb35a2e62a3ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create the `statedicts` folder\n",
    "statedict_dir = Path(\"statedicts\")\n",
    "statedict_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save Q to checkpoints\n",
    "Q_trained = Q\n",
    "\n",
    "# save the trained Q table\n",
    "jnp.save(str(statedict_dir / \"task_3-10-3_trained_Q_table.npy\"), Q_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46b9a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1aad91cc4465d91e9d64ed318a13ddea",
     "grade": false,
     "grade_id": "cell-ba80614d46c27ec4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3.11 - Inference & analysis (9p)\n",
    "\n",
    "### Task 3.11.1: Plotting of cumulative reward and episode duration\n",
    "\n",
    "First, please plot the evolution of the cumulative reward and the duration of a epsiode over the training process. **Hint:** Implement a moving average / running mean to reduce the noise in the plot and make later analysis easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d0f26",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a61a606138c73a089a1f60af6e2c76b",
     "grade": true,
     "grade_id": "task-3-11-1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# now we analyse the cumulative reward and the episode duration as a function of the episode\n",
    "\n",
    "\n",
    "# Hint: implement a function to compute the moving average,\n",
    "# which makes it easier to detect trends\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# we make two subplots\n",
    "# 1. Left: plot the cumulative reward (y-axis) vs. the episode (x-axis)\n",
    "# 2. Right: plot the episode duration [s] (y-axis) vs. the episode (x-axis)\n",
    "plt.figure(figsize=(10.0, 4.8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cumulative reward\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Episode duration [s]\")\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(str(outputs_dir / \"task_3-11-1_cumulative_reward_and_episode_duration.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ec96c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d57f4814d33a978683dd40a82d6212ca",
     "grade": false,
     "grade_id": "cell-e5e8ab5e7d10db09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.11.2: Use the trained policy for closed-loop control (3p)\n",
    "\n",
    "Now, its time to use the trained policy for closed-loop control of the single pendulum. Please make sure that any exploration is deactivated. First, implement a feedback controller below, which uses the trained RL policy to compute a torque to be applied to the pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835f30d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9404318cae5d2685f9a978816872fb02",
     "grade": true,
     "grade_id": "cell-6de16995a5230b7c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def ctrl_fb_rl(\n",
    "    th: Array,\n",
    "    th_d: Array,\n",
    "    th_des: Array,\n",
    "    th_d_des: Array,\n",
    "    Q: Array,\n",
    "    par: Dict,\n",
    "    rng: random.KeyArray,\n",
    ") -> Array:\n",
    "    \"\"\"\n",
    "    RL controller\n",
    "    Args:\n",
    "         th: joint angles [rad] of shape (2, )\n",
    "         th_d: joint velocities [rad/s] of shape (2, )\n",
    "         th_des: desired joint angles [rad] of shape (2, )\n",
    "         th_d_des: desired joint velocities [rad/s] of shape (2, )\n",
    "         Q: trained Q table of shape (N, M, O)\n",
    "         par: dictionary with RL parameters\n",
    "         rng: PRNG key for pseudo-random number generation.\n",
    "     Returns:\n",
    "         tau: Torques [Nm] to be applied to the pendulum links computed by the RL controller of shape (2, )\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    tau = jnp.array([torque, 0.0])\n",
    "\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f87c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SARSA_PAR_INFERENCE = SARSA_PARAMS.copy()\n",
    "# deactivate exploration for inference\n",
    "SARSA_PAR_INFERENCE[\"explore\"] = False\n",
    "\n",
    "# set the random seed (shouldn't be used by code, but interface requires it)\n",
    "key = random.PRNGKey(548815)\n",
    "\n",
    "# set the initial conditions\n",
    "th_0 = jnp.array([-0.5 * jnp.pi / 2, 0.0])\n",
    "th_d_0 = jnp.array([0.0, 0.0])\n",
    "\n",
    "# define duration of simulation\n",
    "sim_duration = 20.0  # [s]\n",
    "t_ts = dt * jnp.arange(int(sim_duration / dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7dabc1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b05ff67ed1160fd63d6c2ad98c52d5d0",
     "grade": true,
     "grade_id": "cell-4a1816eff278edaf",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# setup the RL controller\n",
    "# the final RL control law needs to conform to the interface:\n",
    "#   ctrl_fb_fn(th, th_d, th_des, th_d_des) -> tau\n",
    "# Hint: make use of `partial`\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053bacac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12da5b2ee4f3327162160439110acff2",
     "grade": false,
     "grade_id": "cell-07ea080f6051a0c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# simulate the robot\n",
    "sim_ts = simulate_robot(\n",
    "    rp=ROBOT_PARAMS_SINGLE,\n",
    "    t_ts=t_ts,\n",
    "    th_0=th_0,\n",
    "    th_d_0=th_d_0,\n",
    "    th_des_ts=jnp.pi / 2 * jnp.ones((t_ts.shape[0], 2)),\n",
    "    th_d_des_ts=jnp.zeros((t_ts.shape[0], 2)),\n",
    "    ctrl_fb=ctrl_fb_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ebda9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "084db32a958ac991efc559226c3b631a",
     "grade": false,
     "grade_id": "cell-07983e0b3ad348d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the evolution of the single pendulum in configuration space\n",
    "plot_configuration_space_evolution(\n",
    "    sim_ts,\n",
    "    filepath=str(\n",
    "        outputs_dir / \"task_3-11-2_controllled_configuration_space_evolution.pdf\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49843d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f76a05292e5850a02deed59a0bb7509e",
     "grade": false,
     "grade_id": "cell-15bc2f1d51b0a41f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the evolution of the single pendulum in operational space\n",
    "plot_operational_space_evolution(\n",
    "    sim_ts,\n",
    "    filepath=str(\n",
    "        outputs_dir / \"task_3-11-2_controlled_operational_space_evolution.pdf\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d186a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21cccbd73181ca890e6ea8922f234a8c",
     "grade": false,
     "grade_id": "cell-9ec9643b517a7f07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not AUTOGRADING:\n",
    "    from IPython.display import display, HTML  # For animations in the notebook\n",
    "\n",
    "    ani = animate_robot(\n",
    "        ROBOT_PARAMS_SINGLE,\n",
    "        sim_ts=sim_ts,\n",
    "        step_skip=2,\n",
    "        show=False,\n",
    "        filepath=str(outputs_dir / \"task_3-11-2_controlled_robot.mp4\"),\n",
    "    )\n",
    "    display(HTML(ani.to_html5_video()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ceed8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66201c189a1b4048cbd83b05726f2ac5",
     "grade": false,
     "grade_id": "cell-07e15423bf83ec86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.11.3 Analysis of the RL training (4p)\n",
    "\n",
    "Analyze:\n",
    "- How many simulation steps on average does a swing-up take (after learning has finished)? Will it\n",
    "be wise to reduce the number of steps per trial during learning?\n",
    "- Large parts of the policy in the policy graph are quite noisy. What reasons can you name?\n",
    "- Test your code with greedy and ε-greedy policies. Which method allows the algorithm to converge\n",
    "faster and which method results in a higher cumulative reward (on average)? Can SARSA converge with constant ε? Explain the reason.\n",
    "- Try several values of the discount rate, spanned across the [0, 1) interval. What discount rate allows\n",
    "the algorithm to converge faster? Explain the reason.\n",
    "- Is your policy capabale of swinging up the pendulum? Is your policy capable of stabilizing the pendulum at the top? What can be changed in the training to improve capabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b126fb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f2ca7760c2523ba76e61d85e0b25af67",
     "grade": false,
     "grade_id": "cell-659cd1d7bf64f227",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write up your observations as bulletpoints (maximum 200 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af15782",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d21cfc446a25b940bef0e49b78ba5508",
     "grade": false,
     "grade_id": "task-3-11-3",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2f396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8f926c7cd6b2fea0da8752303fc36e3cccd6cd5a6941f00a5d189c50e5969dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
